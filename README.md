# Tokenization 🗝️🔢📚

## 🚀 Project Overview
Welcome to the Tokenization project, a C++ developed solution adept in handling text-to-number transformations, akin to the tokenization process used in Large Language Models (LLMs) like ChatGPT. This repository presents a hash table integrated with linked lists, primarily designed for efficient word-token mappings, mirroring the tokenization stage in LLMs where text is converted into numbers for computational processing.

## 📌 Key Features
- **Tokenization Emphasis**: Mirrors the tokenization process in LLMs, converting text to numeric tokens and vice versa, crucial for NLP applications.
- **Dynamic Hash Table**: Adapts size dynamically for varying data loads, maintaining efficient performance.
- **Word-Token Mapping**: Manages word and corresponding token insertions and retrievals, pivotal in text analysis.
- **File Interaction**: Reads text from files, tokenizes content, and incorporates it into the hash table.
- **Interactive Commands**: Offers a user-friendly command interface for diverse hash table operations.

## 🛠️ Built With
- **C++**: Forms the core of the project, offering a strong foundation for data structure implementation.
- **Custom Linked List**: Enhances data storage and retrieval, managing collisions effectively.

## 🎯 Application Scenarios
- **NLP and Text Processing**: Ideal for applications requiring tokenization, like LLMs.
- **Educational Insights**: Learn about hash tables and tokenization processes in computational linguistics.
- **Software Development**: Integrates into larger projects for efficient data handling.

## 🔍 Inside the Code
- **Tokenization**: Converts text to numerical tokens, akin to the process in LLMs, for efficient processing.
- **Dynamic Handling**: Inserts words, resizes the hash table, and manages data load effectively.
- **File Processing**: Reads and processes text files, adding to the hash table for further manipulation.
- **Data Visualization**: Print specific chains to understand data distribution within the hash table.
